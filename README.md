Enhanced Glyph Recognition System

A state-of-the-art PyTorch implementation for character/glyph recognition using the EMNIST dataset with advanced features including residual connections, learning rate optimization, early stopping, and comprehensive evaluation metrics.

Features

Architecture Improvements

Residual Connections: Better gradient flow and improved training stability

Depthwise Separable Convolutions: Efficient alternative architecture option

Adaptive Pooling: Flexible input size handling

Batch Normalization: Accelerated training and improved generalization

Training Enhancements

Learning Rate Finder: Automatic optimal learning rate discovery

Early Stopping: Prevents overfitting with configurable patience

Multiple Schedulers: Support for ReduceLROnPlateau and CosineAnnealing

Gradient Clipping: Prevents exploding gradients

Weight Decay: L2 regularization via AdamW optimizer

Evaluation & Analysis

Confusion Matrix: Both normalized and raw count versions

Per-Class Metrics: Detailed precision, recall, and F1-scores

Misclassification Analysis: Identify most common error patterns

Training Visualization: Loss, accuracy, and learning rate plots

Development Features

Model Checkpointing: Automatic best model saving

Configuration Management: JSON-based config storage

Reproducibility: Fixed random seeds across runs

Progress Tracking: Real-time training progress with tqdm

Installation

Requirements

Python 3.8+

PyTorch 1.10+

CUDA (optional, for GPU acceleration)

Setup

Clone or download the repository

Install dependencies:

pip install -r requirements.txt 

Usage

Basic Training

Train with default settings (EMNIST-balanced, 47 classes):

python enhanced_glyph_recognition.py 

Advanced Usage

Use EMNIST-byclass (62 classes)

python enhanced_glyph_recognition.py --split byclass 

Run with Learning Rate Finder

python enhanced_glyph_recognition.py --lr-finder 

Custom Training Configuration

python enhanced_glyph_recognition.py \ --split balanced \ --batch-size 256 \ --epochs 100 \ --lr 0.0005 \ --output-dir my_experiment 

Quick Test (Skip Full Evaluation)

python enhanced_glyph_recognition.py --epochs 10 --skip-eval 

Command Line Arguments

ArgumentTypeDefaultDescription--splitstrbalancedEMNIST split: 'balanced' (47) or 'byclass' (62)--batch-sizeint128Batch size for training--epochsint50Number of training epochs--lrfloat0.001Initial learning rate--lr-finderflagFalseRun LR finder before training--output-dirstroutputsDirectory for saving results--seedint42Random seed for reproducibility--skip-evalflagFalseSkip comprehensive evaluation 

Configuration

Default configuration can be modified in the get_default_config() function:

config = { # Data 'emnist_split': 'balanced', 'batch_size': 128, 'val_split': 0.1, # Model 'use_residual': True, 'dropout_rate': 0.4, # Training 'num_epochs': 50, 'learning_rate': 0.001, 'weight_decay': 1e-4, 'scheduler': 'cosine', 'early_stopping_patience': 10, # Data Augmentation 'rotation_degree': 15, 'affine_degree': 10, 'translate': 0.08, } 

Output Files

After training, the following files are saved to the output directory:

Model Files

best_model.pth - Best model checkpoint (includes optimizer state)

latest_checkpoint.pth - Latest model checkpoint

config.json - Training configuration

Visualizations

training_history.png - Loss, accuracy, and LR plots

confusion_matrix_normalized.png - Normalized confusion matrix

confusion_matrix.png - Raw count confusion matrix

per_class_accuracy.png - Horizontal bar chart of class accuracies

lr_finder.png - Learning rate finder plot (if used)

Reports

classification_report.txt - Detailed per-class metrics

misclassification_analysis.txt - Top misclassification pairs

Model Architecture

Enhanced CNN with Residual Blocks

Input (1Ã—32Ã—32) â†“ Initial Conv (1â†’32) â†“ Stage 1: ResBlock (32â†’64â†’64) + Downsample â†“ Stage 2: ResBlock (64â†’128â†’128) + Downsample â†“ Stage 3: ResBlock (128â†’256â†’256) + Downsample â†“ Global Average Pooling â†“ FC (256â†’512) + Dropout(0.4) â†“ FC (512â†’num_classes) 

Key Components

ResidualBlock: Each block contains:

Conv3Ã—3 + BatchNorm + ReLU

Conv3Ã—3 + BatchNorm

Skip connection

Final ReLU

Regularization:

Batch normalization after each convolution

Dropout (0.4 and 0.3) in classifier

Weight decay (1e-4) in optimizer

Gradient clipping (max_norm=1.0)

Performance

Expected Results

EMNIST-Balanced (47 classes)

Training accuracy: ~95%+

Validation accuracy: ~88-92%

Test accuracy: ~88-91%

EMNIST-ByClass (62 classes)

Training accuracy: ~92%+

Validation accuracy: ~85-88%

Test accuracy: ~85-87%

Training Time

CPU: ~30-45 min/epoch (depends on CPU)

GPU (RTX 3080): ~2-3 min/epoch

Total training (50 epochs): 1.5-2.5 hours on GPU

Advanced Features

Learning Rate Finder

The LR finder implements Leslie Smith's method to find optimal learning rates:

Starts with very low LR (1e-7)

Gradually increases to high LR (10)

Records loss at each step

Suggests LR at steepest descent point

Usage:

python enhanced_glyph_recognition.py --lr-finder 

The tool will:

Generate a plot showing LR vs Loss

Suggest an optimal LR

Prompt you to accept or reject the suggestion

Early Stopping

Automatically stops training when validation loss stops improving:

Default patience: 10 epochs

Monitors validation loss

Restores best model weights

Configurable via config dict

Learning Rate Schedulers

CosineAnnealing (default):

Smooth LR decay with warm restarts

Better final performance

Less hyperparameter tuning needed

ReduceLROnPlateau:

Reduces LR when validation loss plateaus

More conservative

Good for stability

Customization

Using Your Own Dataset

Modify the get_data_loaders() function:

def get_data_loaders(config): train_transform = transforms.Compose([...]) # Replace with your dataset train_dataset = YourDataset( root='./data', train=True, transform=train_transform ) # Rest of the code remains the same ... 

Modifying Architecture

Change model hyperparameters:

model = EnhancedGlyphCNN( num_classes=num_classes, dropout_rate=0.5, # Increase dropout use_residual=False # Use depthwise separable conv instead ) 

Adding Custom Augmentations

Modify transforms in get_data_loaders():

train_transform = transforms.Compose([ transforms.Pad(2), transforms.RandomRotation(20), # Increase rotation transforms.RandomAffine(degrees=15, translate=(0.1, 0.1)), transforms.ColorJitter(brightness=0.2), # Add color jitter transforms.ToTensor(), transforms.Normalize((mean,), (std,)) ]) 

Troubleshooting

CUDA Out of Memory

Reduce batch size: --batch-size 64

Use CPU: Set device = torch.device('cpu')

Poor Performance

Try learning rate finder: --lr-finder

Increase epochs: --epochs 100

Adjust data augmentation in config

Check for class imbalance in dataset

Slow Training

Increase batch size (if memory allows)

Use GPU if available

Reduce number of workers if CPU-bound

Enable torch.backends.cudnn.benchmark = True

Code Structure

enhanced_glyph_recognition.py â”œâ”€â”€ Model Architecture â”‚ â”œâ”€â”€ ResidualBlock â”‚ â”œâ”€â”€ DepthwiseSeparableConv â”‚ â””â”€â”€ EnhancedGlyphCNN â”œâ”€â”€ Training Components â”‚ â”œâ”€â”€ LRFinder â”‚ â”œâ”€â”€ EarlyStopping â”‚ â””â”€â”€ Trainer â”œâ”€â”€ Evaluation â”‚ â””â”€â”€ Evaluator â”œâ”€â”€ Data Loading â”‚ â””â”€â”€ get_data_loaders() â””â”€â”€ Main Pipeline â””â”€â”€ main() 

License

MIT License - Feel free to use and modify for your projects.

Citation

If you use this code in your research, please cite:

@software{enhanced_glyph_recognition, title={Enhanced Glyph Recognition System}, author={Claude AI}, year={2026}, url={https://github.com/yourusername/enhanced-glyph-recognition} } 

Acknowledgments

EMNIST dataset: Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017)

Learning rate finder: Smith, L. N. (2017). Cyclical Learning Rates for Training Neural Networks

PyTorch framework: Paszke, A., et al. (2019)

Contact

For questions or issues, please open an issue on GitHub or contact the maintainer.

Happy Training! ðŸš€

